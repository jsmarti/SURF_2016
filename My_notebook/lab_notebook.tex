\documentclass{article}
\usepackage[margin=0.4in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\hyphenpenalty=10000
\hbadness=10000

\title{SURF 2016 Lab Notebook}
\author{Juan Sebastían Martínez}
\date{}
\begin{document}
\maketitle
\hrulefill

\section{Bayesian Inference (Taken from Probabilistic programming and Bayesian methods for Hackers)}

Bayesian thinking preserves uncertainty and probability as a measure of belief or confidence of an event to occur. This definition leaves room for conflicting beliefs, as different individuals may have different beliefs of events occuring, depending on the information they have about the world.\\

In this context, $P(A)$ denotes the belief about the occurrence of event $A$ and its called the \textit{prior probability}, but after seeing evidence, the belief can change and we denote $P(A|X)$ as the probability of $A$ given the evidence $X$, this is called the \textit{posterior probability}. Bayesian statistics don't return exact values, they return probabilities of ocurrence by the introduction of priors, this preserves \textit{uncertainty}. From the Bayesian point of view, observations of events update the kowledge and modify the posterior.\\

\subsection{Probability Distributions}

For any random variable $Z$, a probability distribution is a function that assigns a probability to any outcome of $Z$. $Z$ can be a \textbf{discrete} variable, \textbf{continuous} or \textbf{mixed} (combination of both). The expected value (EV) is the mean value in the long run for many repeated samples from that distribution.\\

If the random variable $Z$ is discrete, its probability density function is called a \textit{probability mass function}. A very common distribution is the poisson distribution:\\

\begin{center}
	$Z \sim Poi(\lambda)$\\
	\vspace{0.2cm}
	$P(Z = k) = \cfrac{\lambda^{k}e^{-\lambda}}{k!}, k = 0,1,2,3,\cdots$\\
	\vspace{0.2cm}
	$E[Z|\lambda] = \lambda$
\end{center}.

When the random variable is continuous, it has a \textit{probability density function}. One example is the exponencial distribution:\\

\begin{center}
	$Z \sim Exp(\lambda)$\\
	\vspace{0.2cm}
	$f_{Z}(Z|\lambda) = \lambda e^{-\lambda z}, z >= 0$\\
	\vspace{0.2cm}
	$E[Z|\lambda] = \cfrac{1}{\lambda}$
\end{center}

In Bayesian statistics, there has to be a belief about what $\lambda$ might be, so it is given a probability distribution. This modeling often depend on \textit{hyper-parameters}, which are parameters that control other parameters, hyper-parameters are often called \textit{parent} parameters and controlled parameters are \textit{child parameters}.

%From this point of view, there are some useful rules with interesting meanings:\\

%\begin{center}
	%$P(AB) = P(A|B)P(B) = P(B|A)P(A)$\\
	%\vspace{0.2cm}
	%Bayes Rule:\\
	%\vspace{0.2cm}
	%$P(B|A)=\cfrac{P(A|B)P(B)}{P(A)}$.
%\end{center}

%If $A$ and $B$ are independent, $P(A|B) = P(A)$ and $P(B|A) = P(B)$. 

\end{document}