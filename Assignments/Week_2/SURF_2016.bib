Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Huang2009,
abstract = {In this paper, we propose a Multiobjective Self-adaptive Differential Evolution algorithm with objective-wise learning strategies (OW-MOSaDE) to solve numerical optimization problems with multiple conflicting objectives. The proposed approach learns suitable crossover parameter values and mutation strategies for each objective separately in a multi-objective optimization problem. The performance of the proposed OW-MOSaDE algorithm is evaluated on a suit of 13 benchmark problems provided for the CEC2009 MOEA Special Session and Competition (http://www3.ntu.edu.sg/home/epnsugan/) on Performance Assessment of Constrained / Bound Constrained Multi-Objective Optimization Algorithms.},
author = {Huang, V L and Zhao, S Z and Mallipeddi, R and Suganthan, P N},
doi = {10.1109/CEC.2009.4982947},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/huang{\_}2.pdf:pdf},
isbn = {9781424429592},
journal = {2009 IEEE Congress on Evolutionary Computation (CEC'2009)},
pages = {190--194},
title = {{Multi-Objective Optimization Using Self-Adaptive Differential Evolution Algorithm}},
year = {2009}
}
@book{Seeger2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {Seeger, Matthias},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
eprint = {026218253X},
file = {:Users/sebastian/Documents/SURF{\_}2016/Books{\_}and{\_}Tutorials/RW.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
keywords = {2006,c,c 2006 massachusetts institute,e,gaussian processes for machine,gaussianprocess,gpml,i,isbn 026218253x,k,learning,of technology,org,rasmussen,the mit press,williams,www},
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
url = {http://www.gaussianprocess.org/gpml/chapters/RW.pdf},
volume = {14},
year = {2004}
}
@article{Davidson-Pilon2014,
author = {Davidson-Pilon, C},
title = {{Probabilistic programming and bayesian methods for hackers}},
url = {https://scholar.google.com/scholar?q=probabilistic+programming+and+bayesian+methods+for+hackers{\&}hl=en{\&}as{\_}sdt=0,15{\#}0},
year = {2014}
}
@article{Wang2016,
abstract = {An important task of uncertainty quantification is to identify the probability of undesired events, in particular, system failures, caused by various sources of uncertainties. In this work we consider the construction of Gaussian process surrogates for failure detection and failure probability estimation. In particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. We formulate the problem as an optimal experimental design for Bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. In particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. The accuracy and performance of the proposed method is demonstrated by both academic and practical examples.},
archivePrefix = {arXiv},
arxivId = {1509.04613},
author = {Wang, Hongqiao and Lin, Guang and Li, Jinglai},
doi = {10.1016/j.jcp.2016.02.053},
eprint = {1509.04613},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/wang.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Bayesian inference,Experimental design,Failure detection,Gaussian processes,Monte Carlo,Response surfaces,Uncertainty quantification},
number = {September 2015},
pages = {247--259},
title = {{Gaussian process surrogates for failure detection: A Bayesian experimental design approach}},
volume = {313},
year = {2016}
}
@article{Schaul2011,
author = {Schaul, Tom and Sun, Yi and Wierstra, Dann and Gomez, Fausino and Schmidhuber, J{\"{u}}rgen},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/Schaul.pdf:pdf},
isbn = {9781424478354},
pages = {1343--1349},
title = {{Curiosity-Driven Optimization}},
year = {2011}
}
@article{Li2014,
author = {Li, Binbin and Cheng, Hongtai and Chen, Heping and Jin, Tongdan},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/bin{\_}li.pdf:pdf},
isbn = {9781479943159},
pages = {456--461},
title = {{Modeling Complex Robotic Assembly Process Using Gaussian Process Regression}},
year = {2014}
}
@article{Pandita2016,
abstract = {Design optimization under uncertainty is notoriously difficult when the objective function is expensive to evaluate. State-of-the-art techniques, e.g, stochastic optimization or sampling average approximation, fail to learn exploitable patterns from collected data and require an excessive number of objective function evaluations. There is a need for techniques that alleviate the high cost of information acquisition and select sequential simulations optimally. In the field of deterministic single-objective unconstrained global optimization, the Bayesian global optimization (BGO) approach has been relatively successful in addressing the in- formation acquisition problem. BGO builds a probabilistic surrogate of the expensive objective function and uses it to define an information acquisition function (IAF) whose role is to quantify the merit of making new objective evaluations. Specifically, BGO iterates between making the observations with the largest expected IAF and rebuilding the probabilistic surrogate, until a convergence criterion is met. In this work, we extend the expected improvement (EI) IAF to the case of design optimization under uncertainty wherein the EI policy is reformulated to filter out parametric and measurement uncertainties. To increase the robustness of our approach in the low sample regime, we employ a fully Bayesian interpretation of Gaussian processes by constructing a particle approximation of the posterior of its hyperparameters using adaptive Markov chain Monte Carlo. We verify and validate our approach by solving two synthetic optimization problems under uncertainty and demonstrate it by solving the oil-well-placement problem with uncertainties in the permeability field and the oil price time series.},
archivePrefix = {arXiv},
arxivId = {1604.01147},
author = {Pandita, Piyush and Bilionis, Ilias and Panchal, Jitesh},
eprint = {1604.01147},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/piyush{\_}paper.pdf:pdf},
journal = {arXiv:1604.01147},
pages = {10},
title = {{Extending Expected Improvement for High-dimensional Stochastic Optimization of Expensive Black-Box Functions}},
url = {http://arxiv.org/abs/1604.01147{\#}},
year = {2016}
}
@article{Costa2006,
author = {Costa, Eduardo Oliveira and Pozo, Aurora},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/Oliviera.pdf:pdf},
isbn = {1424401003},
number = {C},
pages = {4832--4837},
title = {{A New Approach to Genetic Programming based on Evolution Strategies}},
volume = {00},
year = {2006}
}
@article{Guo2007,
author = {Guo, Shin-Ming},
doi = {10.1109/ICICIC.2007.20},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/shin-ming.pdf:pdf},
isbn = {0-7695-2882-1},
journal = {Second International Conference on Innovative Computing, Information and Control (ICICIC 2007)},
keywords = {A Fast Multi-Objective Evolutionary Algorithm for},
pages = {324--324},
title = {{A Fast Multi-Objective Evolutionary Algorithm for Expensive Simulation Optimization Problems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4427969},
year = {2007}
}
@article{Huang2006,
abstract = {This paper proposes a new method that extends the efficient global$\backslash$noptimization to address stochastic black-box systems. The method$\backslash$nis based on a kriging meta-model that provides a global prediction$\backslash$nof the objective values and a measure of prediction uncertainty at$\backslash$nevery point. The criterion for the infill sample selection is an$\backslash$naugmented expected improvement function with desirable properties$\backslash$nfor stochastic responses. The method is empirically compared with$\backslash$nthe revised simplex search, the simultaneous perturbation stochastic$\backslash$napproximation, and the DIRECT methods using six test problems from$\backslash$nthe literature. An application case study on an inventory system$\backslash$nis also documented. The results suggest that the proposed method$\backslash$nhas excellent consistency and efficiency in finding global optimal$\backslash$nsolutions, and is particularly useful for expensive systems.},
author = {Huang, D. and Allen, T. T. and Notz, W. I. and Zeng, N.},
doi = {10.1007/s10898-005-2454-3},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/huang.pdf:pdf},
isbn = {0925-5001},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Efficient global optimization,Expected improvement,Kriging,Stochastic black-box systems},
number = {3},
pages = {441--466},
title = {{Global optimization of stochastic black-box systems via sequential kriging meta-models}},
volume = {34},
year = {2006}
}
