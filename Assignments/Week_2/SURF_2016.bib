Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Wang2016,
abstract = {An important task of uncertainty quantification is to identify the probability of undesired events, in particular, system failures, caused by various sources of uncertainties. In this work we consider the construction of Gaussian process surrogates for failure detection and failure probability estimation. In particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. We formulate the problem as an optimal experimental design for Bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. In particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. The accuracy and performance of the proposed method is demonstrated by both academic and practical examples.},
archivePrefix = {arXiv},
arxivId = {1509.04613},
author = {Wang, Hongqiao and Lin, Guang and Li, Jinglai},
doi = {10.1016/j.jcp.2016.02.053},
eprint = {1509.04613},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/wang.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Bayesian inference,Experimental design,Failure detection,Gaussian processes,Monte Carlo,Response surfaces,Uncertainty quantification},
number = {September 2015},
pages = {247--259},
title = {{Gaussian process surrogates for failure detection: A Bayesian experimental design approach}},
volume = {313},
year = {2016}
}
@book{Seeger2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {Seeger, Matthias},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
eprint = {026218253X},
file = {:Users/sebastian/Documents/SURF{\_}2016/Books{\_}and{\_}Tutorials/RW.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
keywords = {2006,c,c 2006 massachusetts institute,e,gaussian processes for machine,gaussianprocess,gpml,i,isbn 026218253x,k,learning,of technology,org,rasmussen,the mit press,williams,www},
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
url = {http://www.gaussianprocess.org/gpml/chapters/RW.pdf},
volume = {14},
year = {2004}
}
@article{Constantine2014,
author = {Constantine, Paul G and Dow, Eric and Wang, Qiqi},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/constantine's{\_}paper.pdf:pdf},
keywords = {active subspace methods,gaussian process,kriging,uncertainty quantification},
number = {4},
pages = {1500--1524},
title = {{ACTIVE SUBSPACE METHODS IN THEORY AND PRACTICE}},
volume = {36},
year = {2014}
}
@article{Schaul2011,
author = {Schaul, Tom and Sun, Yi and Wierstra, Dann and Gomez, Fausino and Schmidhuber, J{\"{u}}rgen},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/Schaul.pdf:pdf},
isbn = {9781424478354},
pages = {1343--1349},
title = {{Curiosity-Driven Optimization}},
year = {2011}
}
@article{Bilionis2016,
abstract = {The prohibitive cost of performing Uncertainty Quantification (UQ) tasks with a very large number of input parameters can be addressed, if the response exhibits some special structure that can be discovered and exploited. Several physical responses exhibit a special structure known as an active subspace (AS), a linear manifold of the stochastic space characterized by maximal response variation. The idea is that one should first identify this low dimensional manifold, project the high-dimensional input onto it, and then link the projection to the output. In this work, we develop a probabilistic version of AS which is gradient-free and robust to observational noise. Our approach relies on a novel Gaussian process regression with built-in dimensionality reduction with the AS represented as an orthogonal projection matrix that serves as yet another covariance function hyper-parameter to be estimated from the data. To train the model, we design a two-step maximum likelihood optimization procedure that ensures the orthogonality of the projection matrix by exploiting recent results on the Stiefel manifold. The additional benefit of our probabilistic formulation is that it allows us to select the dimensionality of the AS via the Bayesian information criterion. We validate our approach by showing that it can discover the right AS in synthetic examples without gradient information using both noiseless and noisy observations. We demonstrate that our method is able to discover the same AS as the classical approach in a challenging one-hundred-dimensional problem involving an elliptic stochastic partial differential equation with random conductivity. Finally, we use our approach to study the effect of geometric and material uncertainties in the propagation of solitary waves in a one-dimensional granular system.},
archivePrefix = {arXiv},
arxivId = {1602.04550},
author = {Bilionis, Ilias and Tripathy, Rohit and Gonzalez, Marcial},
eprint = {1602.04550},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/rohit{\_}paper.pdf:pdf},
title = {{Gaussian processes with built-in dimensionality reduction: Applications in high-dimensional uncertainty propagation}},
url = {http://arxiv.org/abs/1602.04550},
year = {2016}
}
@article{Li2014,
author = {Li, Binbin and Cheng, Hongtai and Chen, Heping and Jin, Tongdan},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/bin{\_}li.pdf:pdf},
isbn = {9781479943159},
pages = {456--461},
title = {{Modeling Complex Robotic Assembly Process Using Gaussian Process Regression}},
year = {2014}
}
@article{Costa2006,
author = {Costa, Eduardo Oliveira and Pozo, Aurora},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/Oliviera.pdf:pdf},
isbn = {1424401003},
number = {C},
pages = {4832--4837},
title = {{A New Approach to Genetic Programming based on Evolution Strategies}},
volume = {00},
year = {2006}
}
@article{Guo2007,
author = {Guo, Shin-Ming},
doi = {10.1109/ICICIC.2007.20},
file = {:Users/sebastian/Documents/SURF{\_}2016/Papers/shin-ming.pdf:pdf},
isbn = {0-7695-2882-1},
journal = {Second International Conference on Innovative Computing, Information and Control (ICICIC 2007)},
keywords = {A Fast Multi-Objective Evolutionary Algorithm for},
pages = {324--324},
title = {{A Fast Multi-Objective Evolutionary Algorithm for Expensive Simulation Optimization Problems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4427969},
year = {2007}
}
