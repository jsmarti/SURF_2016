\documentclass{journal}
\usepackage[margin=0.6in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}

\hyphenpenalty=10000
\hbadness=10000

\title{\textbf{Be The Expert on Your Research Topic}\\Part 2: Writing a Literature Review}
\author{Juan Sebastian Martinez Carvajal}
\date{}

\begin{document}
\maketitle
\hrulefill

\section{\underline{Literature Review}}

A great variety of problems in science and engineering can be expressed as optimization problems, where a function that describes the operation of a system should be minimized or maximized, restricted to some constraints imposed by the system itself. For example, a supermarket may want to minimize its inventory purchases while keeping enough stock for its operation, or in a metal-forming shop, a manufacturer may want to maximize the performance of the manufacturing system adjusting certain parameters, like the forming temperature and the die geometry \cite{Huang2006}. Moreover, real optimization problems do not have trivial or ``easy" solutions, since in many cases the optimization could target: black-box systems where the physics and mathematics of the process are not well known; expensive simulations or evaluations of objective functions \cite{Jones1998}; introduction of uncertainty at the output due to noise in the input data; and multiple objectives. In this context, a need for the design of a method for solving stochastic multi-objective optimization problems arises and becomes important in the science and engineering community.\\

One of the first issues in optimization that arises in real science and engineering problems is the lack of knowledge of the physics that rules the underlying process of the system being studied. This case is treated as a black-box system where limited output observations are available to build a model that could describe the system with the highest accuracy possible. Optimization under these circumstances has been approached from different perspectives, for example, advances in efficient global optimization of expensive simulations have been made through the analysis of various criteria of the problem, like the expected improvement (EI) of running the simulation at a particular point; also, stopping criteria for the sequential optimization processes that rules black-box problems have also been developed \cite{Jones1998}, as well as curiosity-driven techniques \cite{Schaul2011}. In this matter, the structure for solving black-box optimization problems is based on building response surfaces over observed data. The taxonomy of this process includes the use of different criteria to evaluate the sequential process of optimization, in which the EI information acquisition scheme has proven to be more efficient than the other criteria, such as the probability of improvement and lower and upper confidence bounds \cite{Jones2001}.\\

This study shows how black-box systems have been approached in order to perform a sequential process of optimization. However, dealing with a black-box system is only one of the many problems optimization faces in real scientific and engineering scenarios. Another major issue corresponds to the effect of epistemic uncertainty, noise in the input data, and how it propagates in the surrogate models used in optimization. For this problem, the use of Gaussian process regression has become a very useful and effective strategy to generate a stochastic approach for the problem \cite{Wang2016}, modifying the EI criteria and considering Bayesian models to manipulate efficient global optimization schemes, as presented in \cite{Huang2006}, \cite{Pandita2016} and \cite{Li2014}. Stochastic approaches to optimization problems often require the development and implementation of probabilistic machine learning techniques, which are fully developed tools as documented by Seeger \cite{Seeger2004} and Davidson-Pilon \cite{Davidson-Pilon2014}. Although developed tools and models for optimization form the state of the art of stochastic optimization, other great issue arises in the systems studied so far, which is the problem of targeting multiple objectives inside the same system.\\

Multi-objective optimization (MOO) problems are widely studied through different techniques, which include the application of genetic algorithms through the appliance of evolutionary strategies \cite{Costa2006}. In MOO problems, genetic algorithms are used to obtain the Pareto front, which contains the optimal solutions to the problem studied and it is typically the main objective of these type of algorithms, but also, evolutionary techniques have been developed to solve expensive simulation-based optimization problems as presented in \cite{Guo2007} and \cite{Huang2009}. Although these techniques address MOO problems, they do not consider an stochastic approach to handle uncertainty. On the other hand, this type of considerations have been made, and they show how MOO problems can be addressed by an extension of the expected improvement (EI) criteria, deriving a closed form of the information acquisition function \cite{wagner2010expected}. Within this context, the study presented here aims to develop a ``greedy'' and reliable method for MOO problems for real black-box systems, addressing uncertainty propagation and considering expensive objective function evaluations.\\

\section{\underline{Mentor Check}}

After meeting with the corresponding mentors of the project, only grammar corrections were made. A correct understanding of the project and the related concepts was achieved, so there where no changes to the content of the review. Additionally, the structure of the document was correct and there were no suggested changes. 

\bibliographystyle{ieeetr}
\bibliography{SURF_2016}

\end{document}
